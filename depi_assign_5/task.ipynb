{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d37f82",
   "metadata": {},
   "source": [
    "## Mini ETL Project Using Python OOP\n",
    "\n",
    "For Data Engineering Track ‚Äî Practice on Classes, Inheritance & Polymorphism\n",
    "\n",
    "### üìå Task Description\n",
    "\n",
    "In this exercise, you will build a simple ETL pipeline using Object-Oriented Programming concepts.\n",
    "\n",
    "You will simulate receiving raw data from different sources (CSV, JSON, etc.), then process it, then load it.\n",
    "Your goal is to apply:\n",
    "\n",
    "- Classes & Objects\n",
    "\n",
    "- Attributes & Methods\n",
    "\n",
    "- Instance vs Class behavior\n",
    "\n",
    "- Inheritance\n",
    "\n",
    "- Method overriding\n",
    "\n",
    "- Polymorphism\n",
    "\n",
    "This is NOT a real ETL pipeline ‚Äî it‚Äôs a small conceptual example to help you understand how OOP can organize data engineering workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273472a8",
   "metadata": {},
   "source": [
    "## What You Will Build?\n",
    "\n",
    "You will create:\n",
    "\n",
    "1Ô∏è‚É£ Base Class ‚Äì DataSource\n",
    "\n",
    "- Holds raw data\n",
    "\n",
    "- Defines the generic steps: extract(), transform(), and load()\n",
    "\n",
    "- Child classes will override transform() depending on the data type\n",
    "\n",
    "2Ô∏è‚É£ Child Classes:\n",
    "\n",
    "- CSVData ‚Üí simulates cleaning CSV rows\n",
    "\n",
    "- JSONData ‚Üí simulates extracting JSON fields\n",
    "\n",
    "- Each class has its own version of transform() ‚Üí THIS is polymorphism.\n",
    "\n",
    "3Ô∏è‚É£ ETL Runner Function\n",
    "\n",
    "- A single function called run_etl(source) that works with any data source object.\n",
    "\n",
    "- This shows how one interface can work with multiple classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870b57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self,raw_data):\n",
    "        self.raw_data = raw_data\n",
    "        self.data = None\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "class CSVData(DataSource):\n",
    "    def __init__(self, raw_data):\n",
    "        super().__init__(raw_data)\n",
    "\n",
    "    def extract(self):\n",
    "        self.data = list(self.raw_data.split(','))\n",
    "    \n",
    "\n",
    "    def transform(self):\n",
    "        cleaned_data = []\n",
    "        for element in self.data:\n",
    "            cleaned_data.append(element.strip())\n",
    "        self.data = cleaned_data # we had to update the data in the process and this is the idea of our ETL pipeline, that it flows and update it self\n",
    "        \n",
    "\n",
    "    def load(self):\n",
    "        print(self.data)\n",
    "        return self.data\n",
    "\n",
    "\n",
    "class JSONData(DataSource):\n",
    "    def __init__(self, raw_data):\n",
    "        super().__init__(raw_data)\n",
    "\n",
    "    def extract(self):\n",
    "        self.data = self.raw_data.copy() # we do this here so we dont modify the original, this is an extra idea i learnt from chatgpt\n",
    "        return self.data # i do this here so if someone want to extract the data using this method and use the value returned\n",
    "    \n",
    "    def transform(self):\n",
    "        keys = [\"name\",\"age\",\"score\"]\n",
    "        cleaned_data = {}\n",
    "\n",
    "        for key in keys:\n",
    "            value = self.data.get(key)\n",
    "            if value.isdigit():\n",
    "                value = int(value)\n",
    "            cleaned_data[key] = value\n",
    "        \n",
    "        self.data = cleaned_data\n",
    "        return self.data\n",
    "    \n",
    "    def load(self):\n",
    "        print(self.data)\n",
    "        return self.data\n",
    "\n",
    "def run_etl(source):\n",
    "    source.extract()\n",
    "    source.transform()\n",
    "    source.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be324be",
   "metadata": {},
   "source": [
    "so what is remaining now is trying to integrate real csv and json files into this pipeline so its fully functional pipeline, will try to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6732fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
